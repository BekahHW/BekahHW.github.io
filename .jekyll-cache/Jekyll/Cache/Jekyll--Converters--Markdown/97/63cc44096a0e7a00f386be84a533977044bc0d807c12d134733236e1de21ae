I"ø&<p>When we were working on the <a href="https://opensauced.pizza/learn/intro-to-oss">Intro to Open Source</a> course, one of the biggest painpoints we noted with new contributors was the frustration they felt when their PRs weren‚Äôt merged in in what they felt was a reasonable amount of time. They had done their research, found an issue, gotten assigned, and then‚Ä¶nothing. No feedback. No merge. Just silence.</p>

<p>It‚Äôs a familiar story. I‚Äôve had contributors tell me, ‚ÄúMy PR has been sitting there for two weeks and I haven‚Äôt heard a thing.‚Äù And I get it. There are so many reasons this happens, including burnout, abandoned projects, the lottery factor, and it‚Äôs rarely about bad intentions. That‚Äôs why I always tell contributors to join the community before contributing. It helps you understand the project‚Äôs rhythms, how to communicate with maintainers, and whether it‚Äôs a space that supports new members.</p>

<p>If you‚Äôve read anything I‚Äôve written in the last five years, you know I care deeply about the open source community. But how we have traditionally evaluated projects and metrics doesn‚Äôt give enough insight into the most meaningful parts. A lot of times, these metrics‚Äîstars, forks, downloads, and DORA metrics‚Äîmiss the most important part of the story: how people collaborate.</p>

<h2 id="a-different-approach-to-open-source-metrics">A Different Approach to Open Source Metrics</h2>

<p>Since <a href="https://x.com/saucedopen/status/1900339926832734635">OpenSauced shut down</a>, I‚Äôve been exploring different options for understanding the collaboration problem. 
<a href="https://collab.dev/">Collab.dev</a> isn‚Äôt a replacement for OpenSauced, but it‚Äôs telling a different (and important) part of the story and capturing how people collaborate. It surfaces the human patterns behind the code, like review responsiveness, contributor distribution, and merge dynamics. Industry-accepted metrics like DORA are valuable for understanding software delivery performance, but not so much in the human department. They can tell you how fast code gets deployed, but not whether contributors feel supported, welcomed, or left in the dark. Open source is as much about relationships as releases, and we need metrics that reflect that.</p>

<h2 id="the-collaboration-visibility-gap">The Collaboration Visibility Gap</h2>

<p>The problem isn‚Äôt just that our current metrics are incomplete. Vanity metrics have been touted as meaningful indications of the project‚Äôs health, and, to be direct, they‚Äôre just not that important.</p>

<p>If we consider the challenges maintainers face every day, we‚Äôll see that it‚Äôs often difficult to:</p>
<ul>
  <li>Identify which contributors are most likely to become long-term participants.</li>
  <li>Pinpoint exactly where the review process stalls or breaks down.</li>
  <li>Understand if the community environment genuinely feels welcoming to newcomers.</li>
  <li>Distinguish between sustainable growth and problematic scaling.</li>
</ul>

<p>The path isn‚Äôt clearer for potential contributors either. They often struggle to determine:</p>

<ul>
  <li>Whether the project actively reviews and merges community contributions.</li>
  <li>How long it typically takes for contributions to be reviewed.</li>
  <li>Which maintainers are most responsive in the contributor‚Äôs area of interest.</li>
  <li>If there‚Äôs a healthy balance between contributions from the core team and the wider community.</li>
</ul>

<p>For many of us, we have to make a decision about where to invest our time and energy. It can be a real letdown if we‚Äôve invested time and realized we made the wrong decision, coming out of it with nothing to really show for the work we‚Äôve done. For instance, I wanted to learn more about <a href="https://dub.sh/cognee">cognee</a>, an AI Memory management framework, recently, so I created a <a href="https://dub.sh/cognee-collab">collab.dev cognee page</a> to learn more about the collaboration happening. When you first look at <a href="https://dub.sh/cognee-gh">cognee on GitHub</a>, it looks like a growing open source project with a decent star count, active issues, and regular commits. But looking at Collab.dev‚Äôs dashboard, I get a richer story.</p>

<h2 id="the-story-cognees-data-tells-through-collabdev">The Story Cognee‚Äôs Data Tells through Collab.dev</h2>
<h3 id="contributor-distribution">Contributor Distribution</h3>

<p>When we think about good contributor distribution in an open source project, that usually means responsibilities, activity, and knowledge aren‚Äôt tied to a few people. Distribution allows for decreased burnout, project resilience, and creates a more welcoming environment. What we see with cognee is a genuinely balanced project. With 51% of contributions coming from the core team and 49% from the community, cognee has built real community ownership without abandoning maintainer responsibility, and we can make a connection with a collaborative environment and higher motivation to support the project from the community.</p>

<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/neq300a2e09po9wobend.png" alt="Contributor distribution graph" /></p>

<h3 id="pr-lifecycle-metrics">PR Lifecycle Metrics</h3>

<p>It continues to get interesting. The review funnel shows that 88% of PRs receive reviews and 85.2% get approved. That approval rate tells a story about quality control and contributor experience. It suggests that either the project has excellent contribution guidelines that help people submit good PRs, or the maintainers are actively helping contributors improve their work rather than just rejecting it. On top of this, there‚Äôs a quick turnaround with a median response time of 1.9 hours and 42% of reviews happening within an hour. They‚Äôre not waiting three weeks for a review. The maintainers are cultivating a positive contributor experience through their responsiveness.</p>

<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iv2x52ng29yysttfl1e9.jpg" alt="cognee lifecycle metrics" /></p>

<h3 id="what-this-tells-us-about-the-human-story">What this tells us about the Human Story</h3>

<p>As a maintainer, I‚Äôve been in the situation where I don‚Äôt have the capacity to immediately respond to contributors, and sometimes they even have to wait weeks for my response. Obviously, this isn‚Äôt ideal. Usually, what happens is that the person has moved on, they may not respond at all, or they‚Äôre less likely to contribute again. What we see from cognee‚Äôs numbers is that they don‚Äôt have that same problem.</p>

<p>When someone contributes to cognee, they aren‚Äôt left wondering whether or not their efforts are valued. They get fast enough feedback to stay engaged and iterate quickly. Their turnaround time for a review (1.6hrs) is a good way to encourage repeat contributors. Additionally, with a median merge time of 19.5 hours signals to contributors that their work has real and immediate impact. And they‚Äôre able to see their contributions available to users.</p>

<h3 id="the-collaboration-pattern">The Collaboration Pattern</h3>

<p>When you look at these metrics together, they‚Äôre telling a story of intentional collaboration design, a story that thinks about the contributor and maintainer experience. They‚Äôve created systems and habits that make collaboration feel responsive and worthwhile. What‚Äôs telling about this data is also what‚Äôs <em>not</em> happening. We don‚Äôt see any pile-ups of unreviewed PRs. There are no huge gaps between approval and merge. There are no signs of contributor frustration or maintainer overwhelm.</p>

<p>This collaboration story matters, not just to show that cognee looks like a good place to contribute, but because this can become a replicable story. Other projects can learn how to make collaboration feel good for the contributors involved. We can look at the data and the project and better understand what systems and practices created these patterns, and we can reach out to maintainers of projects we admire to ask: How do you build review workflows that are both thorough and fast? How do you maintain quality while staying responsive to community contributions?</p>

<p>Collaboration quality doesn‚Äôt have to be something we guess at. We can learn more through the data and find projects that have the capacity to take contributions from community members. (And if you‚Äôre interested, collab.dev also has a nifty comparison tool. You can check out my <a href="https://dub.sh/cognee-mem0">mem0 v. cognee comparison</a>.)</p>

<h2 id="the-bigger-picture-measuring-what-matters">The Bigger Picture: Measuring What Matters</h2>

<p>We‚Äôre in a stage of open source where complex human dynamics determine whether open source projects succeed or fail. Collaboration metrics can help lead to better outcomes. When we measure collaboration effectively, we can:</p>

<ul>
  <li>Reduce contributor burnout by identifying overwhelmed maintainers</li>
  <li>Increase successful first contributions by directing early contributors to responsive projects</li>
  <li>Build more sustainable projects by understanding what healthy collaboration looks like</li>
  <li>Create feedback loops that help projects improve their community practices</li>
</ul>

<p>In open source, we‚Äôve proven that collaborative development can create incredible value, but we can‚Äôt ignore the sustainability challenges, maintainer burnout, and the difficulty of scaling human collaboration.</p>

<p>Better visibility into collaboration patterns can help us to understand the future health of open source. We need tools that help us understand not just what code exists, but how effectively people work together to create and maintain it.</p>

<p>Open source has always been about people working together. Our metrics should reflect that meaningful work.</p>
:ET